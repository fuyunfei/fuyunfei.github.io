<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://fuyunfei.github.io</id>
    <title>Queen&apos;s Road East</title>
    <updated>2019-09-30T18:53:17.793Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://fuyunfei.github.io"/>
    <link rel="self" href="https://fuyunfei.github.io/atom.xml"/>
    <subtitle>皇后大道西</subtitle>
    <logo>https://fuyunfei.github.io/images/avatar.png</logo>
    <icon>https://fuyunfei.github.io/favicon.ico</icon>
    <rights>All rights reserved 2019, Queen&apos;s Road East</rights>
    <entry>
        <title type="html"><![CDATA[Tensor Smoothness ]]></title>
        <id>https://fuyunfei.github.io/post/how-to-smooth-the-output-image</id>
        <link href="https://fuyunfei.github.io/post/how-to-smooth-the-output-image">
        </link>
        <updated>2019-09-29T21:36:48.000Z</updated>
        <summary type="html"><![CDATA[<p>Total variation loss could be a solution which basically calculate the gradient of the tensor.</p>
]]></summary>
        <content type="html"><![CDATA[<p>Total variation loss could be a solution which basically calculate the gradient of the tensor.</p>
<!-- more -->
<pre><code class="language-python">    def total_variation(self,x):
        assert K.ndim(x) == 4
        if K.image_data_format() == 'channels_first':
            a = x[:, :, :-1, :- 1] - x[:, :, 1:, :-1]
            b = x[:, :, :-1, :-1] - x[:, :, :-1, 1:]
        else:
            a = x[:, :-1, :-1, :] - x[:, 1:, :-1, :]
            b = x[:, :-1, :-1, :] - x[:, :- 1, 1:, :]
        return K.concatenate((a,b))

    def total_variation_loss(self,x):
        assert K.ndim(x) == 4
        if K.image_data_format() == 'channels_first':
            a = K.square(x[:, :, :-1, :- 1] - x[:, :, 1:, :-1])
            b = K.square(x[:, :, :-1, :-1] - x[:, :, :-1, 1:])
        else:
            a = K.square(x[:, :-1, :-1, :] - x[:, 1:, :-1, :])
            b = K.square( x[:, :-1, :-1, :] - x[:, :- 1, 1:, :])
        return K.sum(K.pow(a + b, 1.25))
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Masked Loss]]></title>
        <id>https://fuyunfei.github.io/post/masked-loss-in-tensorflow</id>
        <link href="https://fuyunfei.github.io/post/masked-loss-in-tensorflow">
        </link>
        <updated>2019-09-29T07:24:17.000Z</updated>
        <summary type="html"><![CDATA[<p>use tf.where to indicate the mask</p>
]]></summary>
        <content type="html"><![CDATA[<p>use tf.where to indicate the mask</p>
<!-- more -->
<pre><code class="language-python">import numpy as np
import tensorflow as tf

'''
 ' Huber loss.
 ' https://jaromiru.com/2017/05/27/on-using-huber-loss-in-deep-q-learning/
 ' https://en.wikipedia.org/wiki/Huber_loss
'''
def huber_loss(y_true, y_pred, clip_delta=1.0):
  error = y_true - y_pred
  cond  = tf.keras.backend.abs(error) &lt; clip_delta

  squared_loss = 0.5 * tf.keras.backend.square(error)
  linear_loss  = clip_delta * (tf.keras.backend.abs(error) - 0.5 * clip_delta)

  return tf.where(cond, squared_loss, linear_loss)

'''
 ' Same as above but returns the mean loss.
'''
def huber_loss_mean(y_true, y_pred, clip_delta=1.0):
  return tf.keras.backend.mean(huber_loss(y_true, y_pred, clip_delta))
</code></pre>
]]></content>
    </entry>
</feed>